{"cells":[{"cell_type":"markdown","metadata":{"id":"90EDUtykhtZg"},"source":["# Practice Session 01+02: Data preparation"]},{"cell_type":"markdown","metadata":{"id":"5lsj90IchtZo"},"source":["Data scientists [spend a big chunk of their time preparing data](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/) and this is one of the first steps in any data mining project. This step is normally called **data preparation**.\n","\n","The processes of getting an initial understanding of a dataset and preparing it usually go hand-in-hand, and it is critical to perform them well to obtain valid results later. Plus, you can save time and effort by learning how to do proper data preparation.\n","\n","In this session, we will assume you just received a new dataset and need to do some initial steps with it:\n","\n","1) Exploratory Data Analysis\n","\n","* Calculate basis statistics as mean, median, variance, maximum and minimum\n","* Look at distributions, identify outliers\n","* Calculate correlations between variables\n","\n","2) Feature engineering:\n","\n","* Deal with missing values\n","* Standardize all numerical columns\n","* Convert categorical columns to dummy binary variables\n","* Date and period management\n","* Feature generation\n","\n","*Tip*: This process has several steps. It is tempting to maintain a single variable throughout the entire cleaning process, and do something like `x = x.step1()` then `x = x.step2()`. This will create problems for you because if you go back and re-execute a cell it might fail to operate on already transformed data. A better approach in cases like this where you do not have memory problems, is to do `x1 = x.step1()`, `x2 = x1.step2()` and so on, i.e., create a new variable after each transformation or set of transformations.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"QMA3CoB2htZr"},"source":["Author: <font color=\"blue\">Your name here</font>\n","\n","E-mail: <font color=\"blue\">Your e-mail here</font>\n","\n","Date: <font color=\"blue\">The current date here</font>"]},{"cell_type":"markdown","metadata":{"id":"GprEZVsWhtZt"},"source":["# 0. The dataset\n","\n","The dataset, contained in `device_db.csv` is a 10000 registers of mobile device purchases around 2019. **Each record in the dataset describes a customer that buys a new mobile telephone**. The attributes are defined as follows:\n","\n","0. PURCHASED_DEVICE: the mobile phone bought by the customer\n","1. DEVICE_VALUE: the cost of the mobile phone bought by the customer\n","2. LAST_DEVICE_DATE: the date of the previous mobile device purchase\n","3. DATA\\_TRAFFIC\\_MONTH_(1..6): The Mbps of data traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n","4. VOICE\\_TRAFFIC\\_MONTH_(1..6): The minutes of voice traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n","5. BILLING\\_MONTH\\_(1..6): Billing (USD) in the month (-1...-6) paid by the customer previous to the mobile device purchase\n","6. DEVICE\\_COST\\_MONTH_(1..6): Monthly cost (USD) associated to the mobile device finance in the month (-1...-6) paid by the customer previous to the mobile device purchase: proportion of owner-occupied units built prior to 1940\n","7. LINE\\_ACTIVATION\\_DATE: Date of the activation of the mobile line by the customer\n","8. MONTHS\\_LAST\\_DEVICE: Number of months of the previous mobile device\n","9. DURATION\\_LINE: Number of months since the customer contracted the mobile line\n","10. PREVIOUS\\_DEVICE\\_MODEL: Model of the previous mobile phone\n","11. PREVIOUS\\_DEVICE\\_MANUF: Manufacturer of the previous mobile phone\n","12. PREVIOUS\\_DEVICE\\_BRAND: Brand of the previous mobile phone\n","\n","This dataset will be used in next practices as recommendation engines.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"NwmZZnyohtZv"},"source":["# 1. Exploratory data analysis"]},{"cell_type":"markdown","metadata":{"id":"f_1pRsAEhtZw"},"source":["Exploratory Data Analysis (EDA) allows to us to have an understanding of the dataset from a stadistics perspective, i.e., data distribution and correlation between variables. This is crucial to select the most relevant variables for some purpose.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"IK3CnA8KhtZ3"},"source":["We open the csv file contaning the data using separator \";\" and assign to a dataframe variable (use `read_csv` from the Pandas library)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChGgFbKWhtZ4"},"outputs":[],"source":["# LEAVE AS-IS\n","input_dataset = pd.read_csv(\"device_db.csv\", sep=\",\")"]},{"cell_type":"markdown","metadata":{"id":"fOQFYraBhtZ5"},"source":["## 1.1. Data types and simple statistics"]},{"cell_type":"markdown","metadata":{"id":"aVQTyu-ghtZ6"},"source":["<font size=\"+1\" color=\"red\">In this cell I print the data header (column names) and the first five rows of data.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(input_dataset.columns)\n","print(input_dataset.head())\n","print(input_dataset.tail())"]},{"cell_type":"markdown","metadata":{"id":"j_wZxEllhtZ6"},"source":["Data frames also provide a convenient way of printing data. There are many ways of creating a data frame, one possibility is by creating an array of dictionaries:\n","\n","```python\n","countries = []\n","countries.append({'capital': 'Asuncion', 'country': 'Paraguay'})\n","countries.append({'capital': 'La Paz', 'country': 'Bolivia'})\n","countries_df = pd.DataFrame(countries, columns=['country', 'capital'])\n","display(countries_df)\n","```\n","\n","Create a dataframe named `column_type_df` containing the name of each column, its type and the number of distinct elements in that column. To iterate through the columns of dataframe `df`, use `for column in df.columns`; to determine the type of a column, use `df[column].dtype`; to retrieve the number of distinct elements of that column, use `df[column].nunique()`\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suuOYC82n1iU"},"outputs":[],"source":["print(input_dataset[\"BILLING_MONTH_1\"].dtype, input_dataset[\"BILLING_MONTH_1\"].nunique())"]},{"cell_type":"markdown","metadata":{"id":"pQlkVaYghtZ7"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with your code to create and display a dataframe containing one row per column, with the type of each column and number of distinct elements of that column.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize an empty list to store column information\n","column_info = []\n","\n","# Iterate through the columns of the DataFrame\n","for column in input_dataset.columns:\n","    column_info.append([column, input_dataset[column].dtype, input_dataset[column].nunique()])\n","\n","# Create DataFrame from the list of column information\n","column_type_df = pd.DataFrame(column_info, columns=['Column Name', 'Column Type', 'Distinct Elements'])\n","\n","print(column_type_df)"]},{"cell_type":"markdown","metadata":{"id":"nmVwp679htZ8"},"source":["To obtain a series from a dataframe you can reference an attribute by name, e.g., `input_dataset.DEVICE_VALUE` returns the series of all device values.\n","\n","On a series, you can use functions from [numpy](https://numpy.org/doc/) such as `np.mean`, `np.nanmedian` (median ignoring the NaN), `np.std`, `np.min` and `np.max`.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"NYV5zEtEhtZ8"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to create a dataframe named `stats_df` containing one row per each column of type ``float64`` in the input data, with the name of the column and its mean, median, maximum and minimum.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# I first created an empty list to store the column statistics\n","\n","column_stats = []\n","\n","for columns in input_dataset.columns:\n","    if input_dataset[columns].dtype == 'float64':\n","        column_mean = input_dataset[columns].mean()\n","        column_median = input_dataset[columns].median()\n","        column_std = input_dataset[columns].std()\n","        column_min = input_dataset[columns].min()\n","        column_max = input_dataset[columns].max()\n","\n","        column_stats.append([columns, column_mean, column_median, column_std, column_min, column_max])\n","\n","stats_df = pd.DataFrame(column_stats, columns=['Column Name', 'Column Mean', 'Median', 'Column Standard Deviation', 'Column Minimum', 'Column Maximum'])\n","print(stats_df)"]},{"cell_type":"markdown","metadata":{"id":"_BuA2n6qhtZ9"},"source":["The `describe` function can be used to describe a series. To invoke it simply do `input_dataset.DEVICE_VALUE.describe()`\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"AJqrdDu6htZ-"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to print each column name and then use the `describe` function to print statistics for that column. Include a blank line after each description.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for columns in input_dataset.columns:\n","    describe = input_dataset[columns].describe()\n","    print(describe)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stats_df1 = input_dataset.describe()\n","stats_df1"]},{"cell_type":"markdown","metadata":{"id":"ehZ0YdYOhtZ_"},"source":["## 1.2. Inventory of device models"]},{"cell_type":"markdown","metadata":{"id":"6QwJH56IhtZ_"},"source":["In exploratory data analysis, it is very useful to do an **inventory** or **census** of the possible values of a variable. For us, a census will be a frequency table in which you show the possible values of a variable, and their frequency, in decreasing order of frequency.\n","\n","\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"VMyYgxOthtaA"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to display a census of PREVIOUS_DEVICE_MODEL and PREVIOUS_DEVICE_BRAND. You should create and display a dataframe in each case.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["previous_device_brand_census = input_dataset[\"PREVIOUS_DEVICE_BRAND\"].value_counts()\n","print(previous_device_brand_census)\n","previous_device_model_census = input_dataset[\"PREVIOUS_DEVICE_MODEL\"].value_counts()\n","print(\"\\n\",previous_device_model_census)"]},{"cell_type":"markdown","metadata":{"id":"9U48YiCShtaB"},"source":["# 2. Feature engineering"]},{"cell_type":"markdown","metadata":{"id":"LkVW0FMUhtaB"},"source":["Feature engineering is the process of extracting valuable features from the data. This requires pre-processing, combining, normalizing, and performing other operations on the values of some features.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"hH96kJ_DhtaC"},"source":["## 2.1. Missing values management"]},{"cell_type":"markdown","metadata":{"id":"DqYve4DmhtaC"},"source":["**Not A Number** (NaN) is a generic term to refer to *something that should be a number, but is not*. Usually, the value is either missing completely (\"null\") or contains the wrong type of object, such as a string or a concept such as infinity.\n","\n","To find which columns contain NaN values, you can use the [isna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html) function, as explained, e.g., [here](https://medium.com/dunder-data/finding-the-percentage-of-missing-values-in-a-pandas-dataframe-a04fa00f84ab).\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"dAJfaE3xhtaC"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with your code to print which columns contain at least one NaN value, and how many NaN values are in that column, as well as how many non NaN values are in that column. You should create a dataframe `nan_counter` and display it.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nan_counter= input_dataset.isna()\n","nan_counter"]},{"cell_type":"markdown","metadata":{"id":"aTxr6dgwhtaD"},"source":["The way **NaNs** are managed varies according to the meaning of each variable. In some occasions, registers should be removed, filled with other columns or calculated (imputed).\n","\n","* To delete rows containing a null value, we can use [dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n","* To replace null values, we can use [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n","\n","Please note that these steps should be applied sequentially, i.e., the output of one step should be fed into the next step. You can do, for instance: `df02 = df01.operation(...)` followed by `df03 = df02.operation(...)` and so on.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"cKNcdjxyhtaE"},"source":["<font size=\"+1\" color=\"red\">If there is no **PURCHASED\\_DEVICE**, **DEVICE\\_VALUE**, or **PREVIOUS\\_DEVICE\\_MODEL**, the row is useless to us. Replace this cell with code to remove those rows.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clear_db = input_dataset\n","clear_db = clear_db.dropna(subset=['PURCHASED_DEVICE','DEVICE_VALUE','PREVIOUS_DEVICE_MODEL'])\n","clear_db.info()"]},{"cell_type":"markdown","metadata":{"id":"9AjS_xWfhtaE"},"source":["<font size=\"+1\" color=\"red\">Any NaN value in **DATA\\_TRAFFIC\\_MONTH\\_(1..6)**, **VOICE\\_TRAFFIC\\_MONTH_(1..6)**, **BILLING\\_MONTH_(1..6)**, or **DEVICE\\_COST\\_MONTH\\_(1..6)** should be assumed to be 0. Replace this cell with code to do that imputation.</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_traffic = [col for col in clear_db.columns if (\"DATA_TRAFFIC\" in col) ]\n","voice_traffic = [col for col in clear_db.columns if (\"VOICE_TRAFFIC\" in col) ]\n","billing_month = [col for col in clear_db.columns if (\"BILLING_MONTH\" in col) ]\n","device_cost = [col for col in clear_db.columns if (\"DEVICE_COST\" in col) ]\n","columns_to_clear = data_traffic + voice_traffic+billing_month+device_cost\n","\n","for col in clear_db.columns:\n","    if col in columns_to_clear: \n","        clear_db[col] = clear_db[col].fillna(0) \n","clear_db.info()"]},{"cell_type":"markdown","metadata":{"id":"SLSkm3sPhtaF"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to print the header and the first ten rows after this processing</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(clear_db.shape)\n","clear_db.head(10)"]},{"cell_type":"markdown","metadata":{"id":"MwtNck3chtaG"},"source":["## 2.2. Distributions, outliers, and correlations"]},{"cell_type":"markdown","metadata":{"id":"g1dpNHkqhtaH"},"source":["We will now plot the distributions of some variables and apply some transformations.\n","\n","* You can use [Seaborn library](https://seaborn.pydata.org/) with `kde=False` to create a histogram.\n","* You can use [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) with `kind='box'` to create a boxplot.\n","    \n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"wubIcsqEhtaH"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with a series of cells with code to plot a histogram of **DEVICE\\_VALUE**, **VOICE\\_TRAFFIC\\_MONTH\\_1-3** (include a legend, each month should be a different color),  **BILLING\\_MONTH\\_2**, **DURATION\\_LINE**. Remember to include labels on the x axis and y axis</font>\n","\n","<font size=\"+1\" color=\"red\">Include after each histogram a markdown cell where you indicate if you recognize any specific distribution (normal, exponential, uniform, ...) or any characteristic of the distribution (unimodal, bimodal).</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variables = ['DEVICE_VALUE', 'VOICE_TRAFFIC_MONTH_1', 'VOICE_TRAFFIC_MONTH_2', 'VOICE_TRAFFIC_MONTH_3', 'BILLING_MONTH_2', 'DURATION_LINE']\n","\n","for var in variables:\n","    sns.histplot(data=clear_db, x=var, kde=False, alpha=0.7, label=var)\n","\n","    # Adding labels and title\n","    plt.xlabel('Value')\n","    plt.ylabel('Frequency')\n","    plt.title(var)\n","    # Adding legend\n","    plt.legend()\n","    # Showing plot\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qXSf3X0chtaI"},"source":["Variables having exponential distribution can be processed and visualized better after transforming them, usually by applying the `log(x+1)` function (we want to avoid zeros, hence the +1).\n","    \n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"-MjxcHbShtaT"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to apply **log(x+1)** to **VOICE\\_TRAFFIC\\_MONTH\\_2** and plot its new distribution.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#we transform the variable that is exponential\n","clear_db['log_var'] = clear_db['VOICE_TRAFFIC_MONTH_2'].apply(lambda x: np.log(x+1))\n","sns.histplot(data=clear_db, x='log_var', kde=False, alpha=0.7, label='VOICE_TRAFFIC_MONTH_2')\n","\n","# Adding labels and title\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.title('VOICE_TRAFFIC_MONTH_2')\n","# Adding legend\n","plt.legend()\n","# Showing plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"R7n3hTBAhtaU"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to create a boxplot for variables **DATA\\_TRAFFIC\\_MONTH\\_2**, **VOICE\\_TRAFFIC\\_MONTH\\_2** and **BILLING\\_MONTH\\_2**</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variables = [ 'VOICE_TRAFFIC_MONTH_2', 'DATA_TRAFFIC_MONTH_2', 'BILLING_MONTH_2']\n","\n","for var in variables:\n","    clear_db[var].plot( kind = 'box')\n","\n","    # Adding labels and title\n","    plt.ylabel('Value')\n","    plt.title(var)\n","    # Adding legend\n","    plt.legend()\n","    # Showing plot\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tEVTej0ghtaV"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating which extreme values would you use as threshold for **outliers** in these variables, by looking at these box plots</font>"]},{"cell_type":"markdown","metadata":{"id":"y7F92wHbhtaV"},"source":["In this dataset, there are many dependencies between different attributes, e.g., a large voice traffic will probably be associated with a large data traffic, a more expensive bill, and possibly a more expensive device (`DEVICE_VALUE`).\n","\n","You can use [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to compute a correlation matrix, and [matplotlib.pyplot.matshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.matshow.html) to show this graphically.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"CgyhdKt1htaW"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to calculate the correlation between all traffic attributes (i.e., voice and data), duration line, billing, device cost and device value. Display the result as a table with rows and columns corresponding to columns, and cells indicating correlations. Display the result as an image using ``matshow``</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_traffic = [col for col in clear_db.columns if (\"DATA_TRAFFIC\" in col) ]\n","voice_traffic = [col for col in clear_db.columns if (\"VOICE_TRAFFIC\" in col) ]\n","billing = [col for col in clear_db.columns if (\"BILLING_MONTH\" in col) ]\n","\n","data_voice = data_traffic + voice_traffic\n","data_billing = data_traffic + billing\n","voice_billing = voice_traffic + billing\n","\n","df = clear_db[data_voice]\n","correlation_matrix = df.corr()\n","\n","plt.figure(figsize=(8, 6))\n","plt.matshow(correlation_matrix, cmap='RdBu', fignum=1)\n","plt.colorbar(label='Correlation')\n","plt.xticks(np.arange(len(data_voice)), data_voice, rotation=45)\n","plt.yticks(np.arange(len(data_voice)), data_voice)\n","plt.title('Correlation Between Data and voice Traffic Attributes, ')\n","plt.show()\n","\n","df = clear_db[data_billing]\n","correlation_matrix = df.corr()\n","\n","plt.figure(figsize=(8, 6))\n","plt.matshow(correlation_matrix, cmap='RdBu', fignum=1)\n","plt.colorbar(label='Correlation')\n","plt.xticks(np.arange(len(data_billing)), data_billing, rotation=45)\n","plt.yticks(np.arange(len(data_billing)), data_billing)\n","plt.title('Correlation Between Data Traffic and billing Attributes, ')\n","plt.show()\n","\n","df = clear_db[voice_billing]\n","correlation_matrix = df.corr()\n","\n","plt.figure(figsize=(8, 6))\n","plt.matshow(correlation_matrix, cmap='RdBu', fignum=1)\n","plt.colorbar(label='Correlation')\n","plt.xticks(np.arange(len(voice_billing)), voice_billing, rotation=45)\n","plt.yticks(np.arange(len(voice_billing)), voice_billing)\n","plt.title('Correlation Between voice traffic and billing Attributes, ')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"WOBcoPofhtaX"},"source":["It seems that billing is more correlated to voice traffic than data traffic. Despite of being positive, they are almost 0 so we can not say that there is a positive relationship between them."]},{"cell_type":"markdown","metadata":{"id":"c5yyWJaHhtaX"},"source":["## 2.3. Date management and period calculation"]},{"cell_type":"markdown","metadata":{"id":"MQ3D-ejJhtaY"},"source":["First, we will determine the date of the `LAST_DEVICE_CHANGE` of the last device that was changed in the entire dataset (i.e., the maximum value of the `LAST_DEVICE_CHANGE` column, plus 30 days). We will refer to that date as `latest_change`.\n","\n","Note that `LAST_DEVICE_CHANGE` is expressed as a floating point number in the format `YYYYMMDD.0`, for instance 3 of July of 2018 would be `20180703.0`. Convert to integer first, then to string.\n","\n","As a string, this is formatted according to [strptime](https://www.geeksforgeeks.org/python-datetime-strptime-function/) conventions with format `%Y%m%d`.\n","\n","Use [datetime.datetime.strptime](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) to convert to create object `latest_change` and print it.\n","\n","Next, add 30 days to that date to obtain object `now` (we will assume we are doing this processing 30 days after the latest device change). Use a `datetime.timedelta` object for that.\n","\n","Your output should look like this:\n","\n","```\n","2019-05-01 00:00:00\n","2019-05-31 00:00:00\n","```\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"3-5Ifh6rhtaY"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to create and print `latest_change` and `now`.</font>"]},{"cell_type":"markdown","metadata":{"id":"Lc4MdaDHhtaZ"},"source":["Now, obtain the series corresponding to the last device change, you can do it by using [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) as if you were using `strptime`:\n","\n","```\n","series_converted = pd.to_datetime(dataframe[column_name], format='%Y%m%d')\n","```\n","\n","Now compute the difference between the now and the series_converted.\n","\n","Divide that difference by `30 * datetime.timedelta(days=1)` to obtain the difference in periods of 30 days (approximately one month).\n","\n","Replace the `MONTHS_LAST_DEVICE` column with those differences. You may need to [fill the NaN with zeroes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html), and [convert to type](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) `int`.\n","\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"Bxy0HzwHhtaZ"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code that replaces the **MONTHS_LAST_DEVICE** column to be equal to the difference, in periods of 30 days, between **LAST_DEVICE_CHANGE** and the `now` variable.</font>"]},{"cell_type":"markdown","metadata":{"id":"wF8vL2kdhtaa"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to update the **DURATION_LINE** value to be the difference, in days, between **LINE_ACTIVATION_DATE** and the `now` variable.\n","    \n","Indicate the average of **DURATION_LINE** -- what is that in years, approximately?</font>"]},{"cell_type":"markdown","metadata":{"id":"mlGxPr1Ehtaa"},"source":["\n","## 2.4. Standarization and scaling of numerical variables"]},{"cell_type":"markdown","metadata":{"id":"UZfUNinmhtab"},"source":["Scaling a series involves changing the values. Standardization involves ensuring that the mean is 0 and the standard deviation is 1, while min-max scaling requires that the maximum is 1, the minimum is 0, and all remaining values are linearly interpolated.\n","\n","You can use [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standarize a variable, and [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to perform min-max scaling.\n","\n","The following example shows how to use these:\n","\n","```python\n","test_data = [{'x': -1.0}, {'x': 2.0}, {'x': 3.0}, {'x': 6.0}]\n","test_df = pd.DataFrame(test_data)\n","display(test_df)\n","\n","test_df['x_standardized'] = StandardScaler().fit_transform(test_df[['x']])\n","test_df['x_minmaxscaled'] = MinMaxScaler().fit_transform(test_df[['x']])\n","display(test_df)\n","```\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","std_data_traffic_m_1 = clear_db\n","scaler = StandardScaler()\n","\n","# Extract the column and convert it into a 2D array-like object\n","data_traffic_month_1 = clear_db[[\"DATA_TRAFFIC_MONTH_1\"]]\n","scaler.fit(data_traffic_month_1)\n","\n","# Now, you can transform the column\n","std_data_traffic_m_1[\"DATA_TRAFFIC_MONTH_1\"] = scaler.transform(data_traffic_month_1)\n","#rint(std_data_traffic_m_1[\"DATA_TRAFFIC_MONTH_1\"])\n","std_data_traffic_m_1[\"DATA_TRAFFIC_MONTH_1\"].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df[\"DATA_TRAFFIC_MONTH_1\"] = MinMaxScaler().fit_transform(test_df[[\"DATA_TRAFFIC_MONTH_1\"]])\n","display(test_df['DATA_TRAFFIC_MONTH_1'])\n","test_df[\"DATA_TRAFFIC_MONTH_1\"].describe()\n","sns.histplot(data= test_df, x=\"DATA_TRAFFIC_MONTH_1\", kde=False, alpha=0.7, label=var)\n","# Adding labels and title\n","plt.xlabel('DATA_TRAFFIC_MONTH_1')\n","plt.ylabel('Frequency')\n","plt.title('Histogram')\n","# Adding legend\n","plt.legend()\n","# Showing plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZF9tnpyLhtac"},"source":["## 2.5. Convert categorical columns to dummy binary variables"]},{"cell_type":"markdown","metadata":{"id":"Gs7F3bUVhtad"},"source":["Categorical variables usually need to be transformed into numerical values to apply some machine learning methods.\n","\n","Use [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform a categorical variable to integer values. Example:\n","\n","```python\n","colors_data = [{'color': 'Blue'}, {'color': 'Red'}, {'color': 'Orange'},\n","               {'color': 'Blue'}, {'color': 'Orange'}, {'color': 'Blue'}]\n","colors_df = pd.DataFrame(colors_data, columns=['color'])\n","\n","colors_df['colors_int_encoded'] = LabelEncoder().fit_transform(colors_df['color'])\n","display(colors_df)\n","```\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"9no-iu69htae"},"source":["<font size=\"+1\" color=\"red\">Create variable **PREVIOUS_DEVICE_BRAND_INT_ENCODED** containing an integer encoding of variable **PREVIOUS_DEVICE_BRAND**.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_int = pd.DataFrame(clear_db, columns = [\"PREVIOUS_DEVICE_BRAND\"])\n","brand_int[\"brand_encoded\"] = LabelEncoder().fit_transform(brand_int['PREVIOUS_DEVICE_BRAND'])\n","display(brand_int)"]},{"cell_type":"markdown","metadata":{"id":"Oxb6Td4mhtaf"},"source":["You can use [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to convert a categorical variable to multiple columns using one-hot encoding. Example:\n","\n","```\n","colors_data = [{'color': 'Blue'}, {'color': 'Red'}, {'color': 'Orange'},\n","               {'color': 'Blue'}, {'color': 'Orange'}, {'color': 'Blue'}]\n","colors_df = pd.DataFrame(colors_data, columns=['color'])\n","\n","color_dummies = pd.get_dummies(colors_df['color'], prefix='color_')\n","colors_df_with_dummies = colors_df.join(color_dummies)\n","display(colors_df_with_dummies)\n","```\n","\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"I_c5E6JQhtag"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to convert **PREVIOUS_DEVICE_MANUF** to dummy binary variables.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["manuf_bin = pd.DataFrame(clear_db, columns = [\"PREVIOUS_DEVICE_MANUF\"])\n","manuf_bin[\"PREVIOUS_DEVICE_MANUF_BIN\"]= LabelEncoder().fit_transform(manuf_bin['PREVIOUS_DEVICE_MANUF'])\n","display(manuf_bin)"]},{"cell_type":"markdown","metadata":{"id":"1-9q5XQuhtag"},"source":["## 2.6. Feature generation"]},{"cell_type":"markdown","metadata":{"id":"LUj9AkFwhtah"},"source":["In the current dataset we have a historic of 6 months for data traffic, voice traffic, billing and device cost. Feature generation consists of creating new attributes from the current dataset that can help us to create, e.g., better predictive models.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"y24iTq2dhtah"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to create from the 6 months of **DATA_TRAFFIC\\_MONTH\\_[1-6]**, **VOICE_TRAFFIC\\_MONTH\\_[1-6]**, **BILLING\\_MONTH\\_[1-6]** and **DEVICE_COST\\_MONTH\\_[1-6]**, new columns with the mean, maximum, minimum, range (i.e., difference between maximum and minimum) for each element. For instance, column **DATA_TRAFFIC_MEAN** should contain the average of these six numbers: **DATA_TRAFFIC_MONTH_1**, **DATA_TRAFFIC_MONTH_2**, ..., **DATA_TRAFFIC_MONTH_6**.</font>"]},{"cell_type":"markdown","metadata":{"id":"fKfQWSoGhtai"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code create an additional column **DEVICE_COST_TO_BILLING_RATIO** containing the ratio between **DEVICE_COST_MEAN** and **BILLING_MEAN** and plot its distribution.</font>"]},{"cell_type":"markdown","metadata":{"id":"zs5kNee3htai"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary on the distribution of the variable **DEVICE_COST_TO_BILLING_RATIO**. Can you recognize its distribution?</font>"]},{"cell_type":"markdown","metadata":{"id":"GdI4h-rphtaj"},"source":["## 2.7. Text parsing/processing"]},{"cell_type":"markdown","metadata":{"id":"86tmZNZChtak"},"source":["In machine learning, text processing is a very useful tool that can be used to improve datasets. In some use cases, for instance customer care applications using digital channels as Whatsapp, Facebook, etc..., data scientist teams mainly work with text data.\n","\n","One of the text processing technique is to extract concrete words or tokens from a sentence or documents. Regular expressions are a great tool to extract data trough these patterns.\n","\n","In this dataset, note that **PURCHASED_DEVICE** is a variable that is formed by a \"**device_code**\"+\"**_**\"+\"**manufacture name**\"+\"**  **\"+\"**device model**\". We want to split this variable into its components.\n","\n","Tip: use [str.split](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to separate a string into several parts.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text_parsing_df = pd.DataFrame(clear_db)\n","text_parsing_df = text_parsing_df['PURCHASED_DEVICE'].str.split('_', expand=True)\n","print(text_parsing_df)"]},{"cell_type":"markdown","metadata":{"id":"HQKS57bEhtal"},"source":["## 2.8. Splitting and sampling a dataset"]},{"cell_type":"markdown","metadata":{"id":"6oN2SFsehtal"},"source":["Splitting and sampling dataset are techniques that distribute the original dataset in n-parts. One of the most interesting application of these tools is to separate the dataset to train and test a machine learning model. Meanwhile sampling guarantees same type of data (i.e. distributions), splitting will separate the dataset with the ratio we need. Usually, 80%-20% or 70%-30% splitting ratios are the most common used.\n","\n","Once again, Sklearn library helps to us to cover this necessity through the function [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) which splits a dataset into two parts, which usually will be used for training and testing.\n","\n","<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"]},{"cell_type":"markdown","metadata":{"id":"rbCpSJN0htam"},"source":["<font size=\"+1\" color=\"red\">Replace this cell with code to split the dataset in two separate datasets: one with 70% of the rows and the other with 30% of rows</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df, test_df = train_test_split(clear_db, test_size=0.3, random_state=42)\n","train_df.info()\n","test_df.info()"]},{"cell_type":"markdown","metadata":{"id":"jPHyVAanhtar"},"source":["<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
